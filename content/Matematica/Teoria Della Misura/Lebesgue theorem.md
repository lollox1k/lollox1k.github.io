The average amount by which a function in $L^1(\mathbb{R})$ differs from its values is small almost everywhere on small intervals.[^1]

> [!theorem]
> Let $f \in L^1(\mathbb{R})$. Then 
> $$
> \lim_{t \downarrow 0} \frac 1{2t} \int_{b-t}^{b+t} |f(x)-f(b)|dx = 0
> $$
> for almost evert $b \in\mathbb{R}$.
> > [!proof]-
> > (Sketch) The theorem is obvious for continuous functions (bound with the supremum). The strategy is to approximate any $L^1$ function with a continuous, we can since they are dense$\dots$ $\square$




[^1]: This statement and the main guide of this note is the book MIRA.